# Adversarial Training

Discriminative Neural networks trained on real data can have adversarial examples.
One kind of adversarial examples that we are generating are out of distribution data points which the discriminative neural network classifies with high confidence as belonging to some class.
Our generation method does not ensure that the generated examples are out of distribution.
It only ensures that the generated examples are classified with high confidence as belonging to a desired target class.
It also ensures that it follows the following conditions:
* individual features (pixels) have valid values (between 0 and 1)
* Generated examples are sparse

For MNIST, the real data domain also satisfied the above conditions.
Hence the generated examples are not guaranteed to be OOD for MNIST.
For other domains, such as CIFAR, the real data will typically not be sparse.

## Objective of adversarial training experiments

Our end-goal is to train a neural network which does not have sparse adversarial images.

We train the neural network to recognize adversarial images generated via our method as belonging to fake classes.
However, this trained network may again have sparse adversarial images different from the ones it was trained on.
Hence we generate such images again and retrain the network on these new adversarial images.
We carry on this process until our adversarial image generation method can no longer generate sparse adversarial images. 

## Current adversarial example generation limitations
We use L1 penalization to produce sparse adversarial images.
However, these may not necessarily produce sparse images which are classified with high confidence as a given class.
Or, the produced images may not be sparse. Or both.

- Problem 1) Ensure sparse images
- Problem 2) Ensure image classified with high confidence
- Problem 3) Ensure out of distribution

If at any time during training we detect that our generated images violate any one of these conditions, then we should deem the trained network to be robust to adversarial images generated by our process.

However, there is a caveat to this - maybe our generation process is not good enough - for example, we may not be training for enough number of steps, or our lambda is too high (resulting in low confidence images) or too low (resulting in non-sparse images)
Perhaps we can perform early stopping, with number of recovery steps set very high (say 10k).
We stop when, for example, probability >= 0.9 and sparsity <= 300.

To ensure OOD, one may use a random mask to mask parts of the generated image.
These parts will be held at 0, and gradients will not flow to them.
This may ensure that the generated images have randomness in them, and do not look like the input distribution.
However, this may only ensure one kind of OOD adversarial images, and network trained on such images may still be fooled by other OOD sparse images.

## Metrics to be logged for Adversarial Training

What do we need to verify?

0. Our adversarial image generation method successfully generates sparse adversarial images.
1. Our adversarial image generation method successfully generates OOD images.
2. Our adversarial training ensures that the network can detect adversarial images it was just trained on.
3. Our adversarial training ensure that the network does not forget past images it was trained on.
4. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, but generated via the same process on the most recent network
5. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, but generated via the same process on a past network.
6. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, but generated via the same process on a completely different network, trained on a different dataset of the same distribution.
7. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, and generated via a different process on the same or different dataset coming from the same distribution.

For (1), we should log probability of image belonging to the target class, as well as sparsity of image.
For (2), we don't know what metric 