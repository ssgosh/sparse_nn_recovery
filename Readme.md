# Adversarial Training

Discriminative Neural networks trained on real data can have adversarial examples.
One kind of adversarial examples that we are generating are out of distribution data points which the discriminative neural network classifies with high confidence as belonging to some class.
Our generation method does not ensure that the generated examples are out of distribution.
It only ensures that the generated examples are classified with high confidence as belonging to a desired target class.
It also ensures that it follows the following conditions:
* individual features (pixels) have valid values (between 0 and 1)
* Generated examples are sparse

For MNIST, the real data domain also satisfied the above conditions.
Hence the generated examples are not guaranteed to be OOD for MNIST.
For other domains, such as CIFAR, the real data will typically not be sparse.

## Objective of adversarial training experiments

## Current adversarial example generation limitations
We use L1 penalization to produce sparse adversarial images. However, these may not necessarily produce sparse images which are classified with high confidence as a given class. Or, the produced images may not be sparse. Or both.

- Problem 1) Ensure sparse images
- Problem 2) Ensure image classified with high confidence
- Problem 3) Ensure out of distribution

If we detect that our generated images violate any one of these conditions, then we should deem the trained network to be robust to adversarial images generated by our process.

However, there is a caveat to this - maybe our generation process is not good enough - for example, we may not be training for enough number of steps, or our lambda is too high (resulting in low confidence images) or too low (resulting in non-sparse images)
Perhaps we can do early stopping, with number of recovery steps set very high (say 10k).
We stop when, for example, probability >= 0.9 and sparsity <= 300.

To ensure OOD, one may use a random mask to mask parts of the generated image.
These parts will be held at 0, and gradients will not flow to them.
This may ensure that the generated images have randomness in them, and do not look like the input distribution.

## Metrics to be logged for Adversarial Training

