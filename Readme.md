# Adversarial Training

Discriminative Neural networks trained on real data can have adversarial examples.
One kind of adversarial examples that we are generating are out of distribution data points which the discriminative neural network classifies with high confidence as belonging to some class.
Our generation method does not ensure that the generated examples are out of distribution.
It only ensures that the generated examples are classified with high confidence as belonging to a desired target class.
It also ensures that it follows the following conditions:
* individual features (pixels) have valid values (between 0 and 1)
* Generated examples are sparse

For MNIST, the real data domain also satisfied the above conditions.
Hence the generated examples are not guaranteed to be OOD for MNIST.
For other domains, such as CIFAR, the real data will typically not be sparse.

## Objective of adversarial training experiments

Our end-goal is to train a neural network which does not have sparse adversarial images.

We train the neural network to recognize adversarial images generated via our method as belonging to fake classes.
However, this trained network may again have sparse adversarial images different from the ones it was trained on.
Hence we generate such images again and retrain the network on these new adversarial images.
We carry on this process until our adversarial image generation method can no longer generate sparse adversarial images. 

## Current adversarial example generation limitations
We use L1 penalization to produce sparse adversarial images.
However, these may not necessarily produce sparse images which are classified with high confidence as a given class.
Or, the produced images may not be sparse. Or both.

- Problem 1) Ensure sparse images
- Problem 2) Ensure image classified with high confidence
- Problem 3) Ensure out of distribution

If at any time during training we detect that our generated images violate any one of these conditions, then we should deem the trained network to be robust to adversarial images generated by our process.

However, there is a caveat to this - maybe our generation process is not good enough - for example, we may not be training for enough number of steps, or our lambda is too high (resulting in low confidence images) or too low (resulting in non-sparse images)
Perhaps we can perform early stopping, with number of recovery steps set very high (say 10k).
We stop when, for example, probability >= 0.9 and sparsity <= 300.

To ensure OOD, one may use a random mask to mask parts of the generated image.
These parts will be held at 0, and gradients will not flow to them.
This may ensure that the generated images have randomness in them, and do not look like the input distribution.
However, this may only ensure one kind of OOD adversarial images, and network trained on such images may still be fooled by other OOD sparse images.

## Metrics to be logged for Adversarial Training

What do we need to verify?

0. Our adversarial image generation method successfully generates sparse adversarial images.
1. Our adversarial image generation method successfully generates OOD images.
2. Our adversarial training ensures that the network can detect adversarial images it was just trained on.
3. Our adversarial training ensure that the network does not forget past images it was trained on.
4. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, but generated via the same process on the most recent network
5. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, but generated via the same process on a past network.
6. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, but generated via the **same process** on a completely different network, trained on a different dataset of the same distribution.
7. Our adversarial training ensure that the network can detect adversarial images that it was not trained on, and generated via a **different process** on the same or different dataset coming from the same distribution. For example, use differential evolution to generate these images.
8. Our adversarial training ensure that the final trained network does not have sparse adversarial images.
9. Our adversarial training ensure that the network does well on real training data.
10. Our adversarial training ensure that the network does well on real test data.

* For (0), we should log **average probability of adversarial image belonging to the target class**, as well as **average sparsity of image**. Should also do this **class-wise**.
* For (1), we don't know what metric should be monitored, or if there is a metric other than human inspection.
* For (2), we should log the current **adversarial batch's probability** of belonging to the fake class. Should also log the **loss**. Should also log this for **individual fake classes**.
* For (3), we should keep around all past training images and evaluate on them after each epoch.
  Target should be fake class while evaluation.
  If this is too costly, only evaluate on a sample of 1000 images for each epoch. 
  Log **both aggregate and per-class stats**.
* For (4) and (5), we should generate **test adversarial images** on each epoch, on which the network is not trained.
  We should log the **same stats as for (3)**.
* For (6), 